<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
<html>
<head>
   <meta http-equiv="Content-Type" content="text/html;
					    charset=iso-8859-1">
   <link rel="stylesheet" type="text/css" href="screen.css" />
   <title>ADCS 2009 Programme</title>
</head>

<body>

<h1>ADCS 2009 Programme</h1>

<div class="menu">
  <a href="index.html">Home and CFP</a><br/>
  <a href="location.html">Location</a><br/>
  <a href="paperguide.html">Paper guidelines</a><br/>
  <a href="proceedings.html">Proceedings</a><br/>
  Programme<br/>
  <a href="reg.html">Registration</a><br/>
  <a href="committee.html">Committee</a><br/>
  <a href="contact.html">Contact</a>
</div>

<h2>Programme</h2>

<table cellspacing="0" border="0" cellpadding="3">

  <tr class="talk"><td>9:00-10:00</td>
      <td>Plenary: David Traum</td></tr>

  <tr><td>10:00-10:30</td>
      <td>Coffee &amp; posters</td></tr>

  <tr class="talk"><td>10:30-12:30</td>

      <td>Huizhi Liang, Yue Xu, Yuefeng Li and Richi Nayak.
      <em>Collaborative Filtering Recommender Systems based on Popular
      Tags</em><br/>

      David Newman, Sarvnaz Karimi and Lawrence
      Cavedon. <em>External Evaluation of Topic Models</em><br/>

      Nicholas Sherlock and Andrew Trotman. <em>Id - Dynamic Views on
      Static and Dynamic Disassembly Listings</em><br/>

      Gavin Shaw, Yue Xu and Shlomo Geva. <em>Interestingness Measures
      for Multi-Level Association Rules</em><br/>

      Hilal Al Maqbali, Falk Scholer, James A. Thom and Mingfang
      Wu. <em>Do Users Find Looking at Text More Useful than Visual
      Representations?  A Comparison of Three Search Result
      Interfaces</em>
      </td></tr>

  <tr><td>12:30-2:00</td>
      <td>Lunch</td></tr>

  <tr class="talk"><td>2:00-3:00</td>
      <td>Plenary: Mark Sanderson.  <em>Is this document relevant?
      Errr it'll do</em></td></tr>

  <tr><td>3:00-3:30</td>
      <td>Coffee &amp; posters</td></tr>

  <tr class="talk"><td>3:30-5:30</td>

      <td>Chris De Vries, Lance De Vine and Shlomo Geva. <em>Random
      Indexing K-tree</em><br/>

      Andrew Turpin and Falk Scholer. <em>Modelling Disagreement
      Between Judges for Information Retrieval System
      Evaluation</em><br/>

      Andrew Trotman and David Alexander. <em>University Student Use
      of the Wikipedia</em><br/>

      Tim O'Keefe and Irena Koprinska. <em>Feature Selection and
      Weighting in Sentiment Analysis</em><br/>

      Su Nam Kim, Timothy Baldwin and Min-Yen Kan. <em>The Use of
      Topic Representative Words in Text Categorization</em>
      </td></tr>

  <tr><td>5:30</td>
      <td>Close</td></tr>
</table>

<h2>Plenary</h2>

<h3>Mark Sanderson</h3>

<p><em>"Is this document relevant? Errr it'll do"</em></p>

<p>Evaluation of search engines is a critical topic in the field of
information retrieval. Doing evaluation well allows researchers to
quickly and efficiently understand if their new algorithms are a
valuable contribution or if they need to go back to the drawing
board. The modern methods used for evaluation developed by
organizations such as TREC in the US have their origins in research
that started in the early 1950s. Almost all of the core components of
modern testing environments, known as test collections, were present
in that early work. Potential problems with the design of these
collections were described in a series of publications in the 1960s,
but the criticisms were largely ignored. However, in the past decade a
series of results were published showing potentially catastrophic
problems with a test collection's "ability" to predict the way that
users will work with searching systems. A number of research teams
showed that users given a good system (as measured on a test
collection) searched no more effectively than users given one that was
bad.</p>

<p>In this talk, I will briefly outline the history of search
evaluation, before detailing the work finding problems with test
collections. I will then describe some pioneering but relatively
overlooked research that pointed out that the key problem for
researchers isn't the question of how to measure searching systems
accurately, the problem is how to accurately measure people.</p>

<h2>Posters</h2>

<ul>

  <li>Ling-Xiang Tang, Shlomo Geva, Andrew Trotman and Yue
  Xu. <br/><em>N-Gram Word Segmentation for Chinese Wikipedia Using
  Phrase Mutual Information</em></li>

  <li>Ming Liu and Rafael A. Calvo. <br/><em>An Automatic Question
  Generation Tool for support Sourcing and Integration in Students'
  Essays</em></li>

  <li>Marco Lui and Timothy Baldwin. <br/><em>You Are What You Post:
  User-level Features in Threaded Discourse</em></li>

  <li>Gavin Shaw, Yue Xu and Shlomo Geva. <br/><em>Investigating the
  use of Association Rules in Improving Recommender Systems</em></li>

  <li>Wei Che (Darren) Huang, Andrew Trotman and Shlomo
  Geva. <br/><em>The Methodology of Manual Assessment in the
  Evaluation of Link Discovery</em></li>

  <li>Tom Rowlands, Paul Thomas and Stephen Wan. <br/><em>Web Indexing
  on a Diet: Template Removal with the Sandwich Algorithm</em></li>

  <li>Liang-Chun Jack Tseng, Dian Tjondronegoro and Amanda
  Spink. <br/><em>Analyzing Web Multimedia Query Reformulation
  Behavior</em></li>

  <li>Michiko Yasukawa and Hidetoshi Yokoo. <br/><em>Term Clustering
  based on Lengths and Co-occurrences of Terms</em></li>

  <li>Vilaythong Southavilay, Kalina Yacef and Rafael
  A. Calvo. <br/><em>WriteProc: A Framework for Exploring
  Collaborative Writing Processes</em></li>

  <li>Sally Jo Cunningham and Simon Laing. <br/><em>An Analysis of
  Lyrics Questions on Yahoo! Answers: Implications for Lyric / Music
  Retrieval Systems</em></li>

  <li>Xiuzhen Zhang, Zhixin Zhou and Mingfang Wu. <br/><em>Positive,
  Negative, or Mixed? Mining Blogs for Opinions</em></li>

</ul>

<div class="footer">
<p>Webmaster: <a href="mailto:paul.thomas@csiro.au">Paul
Thomas</a><br/>
Acknowledgement: This page was shamelessly ripped off from the one
Alistair Moffat edited in 2004 as created by Dave Hawking in 2003.</p>
</div>

</body>
</html>
